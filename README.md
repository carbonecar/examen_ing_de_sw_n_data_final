
# Medallion Architecture Demo (Airflow + dbt + DuckDB) > ‚úèÔ∏è **MODIFICADO:** Esta secci√≥n fue actualizada



Este proyecto crea un pipeline de 3 pasos que replica la arquitectura medallion:

1. **Bronze**: Airflow lee un CSV crudo seg√∫n la fecha de ejecuci√≥n y aplica una limpieza b√°sica con Pandas guardando un archivo parquet limpio.
2. **Silver**: Un `dbt run` carga el parquet en DuckDB y genera modelos intermedios.
3. **Gold**: `dbt test` valida la tabla final y escribe un archivo con el resultado de los data quality checks.

## Estructura

```
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ medallion_medallion_dag.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transactions_20251205.csv
‚îÇ   ‚îú‚îÄ‚îÄ clean/
‚îÇ   ‚îî‚îÄ‚îÄ quality/
‚îú‚îÄ‚îÄ dbt/
‚îÇ   ‚îú‚îÄ‚îÄ dbt_project.yml
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ staging/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ marts/
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îî‚îÄ‚îÄ transformations.py
‚îú‚îÄ‚îÄ profiles/
‚îÇ   ‚îî‚îÄ‚îÄ profiles.yml
‚îú‚îÄ‚îÄ warehouse/
‚îÇ   ‚îî‚îÄ‚îÄ medallion.duckdb (se genera en tiempo de ejecuci√≥n)
‚îî‚îÄ‚îÄ requirements.txt
```

## Requisitos ![Nuevo](https://img.shields.io/badge/NUEVO-brightgreen)
Se puede correr con docker o manual es indistinto. Se recomienda probar con docker puesto que es mas simple
### Ejecuci√≥n con Docker

Es importante cuando se hace docker compose up esperar a que  termine de levantar, si se hace -d no se puede ver con claridad cuando esto pasa y  el 
docker esta hecho para levantar airflow directamente.
```
docker-compose build --no-cache
docker-compose up 
```
Una vez que termino de levantar ir a http://localhost:8080 e iniciar el dag porque aparece deshabilitado. Este comenzara a cargar todos los archivos que hay en raw. 
Si hay alg√∫n problema de loguin probar en modo inc√≥gniot y/o borrar cookies porque airflow deja cookies y sessiones que complican el login.

Para mas documentacion leer [Documentaci√≥n Entrega](doc_entrega/Readme.md)

### Ejecuci√≥n manual
- Python 3.10+
- DuckDB CLI opcional para inspeccionar la base.

Instala dependencias:

```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

## Configuraci√≥n de variables de entorno > ‚úèÔ∏è **MODIFICADO:** Esta secci√≥n fue actualizada


üÜï **NUEVO:** Se agregan dos variables de enotorno nuevas

Se proporciona un script (var_entorno) para facilitar su seteo en la version manual, en la version dockerizada se hace autom√°gicamente. 

```bash
export AIRFLOW_HOME=$(pwd)/airflow_home
export DBT_PROFILES_DIR=$(pwd)/profiles
export DUCKDB_PATH=$(pwd)/warehouse/medallion.duckdb
export AIRFLOW__CORE__DAGS_FOLDER=$(pwd)/dags
export AIRFLOW__CORE__LOAD_EXAMPLES=False
export PYTHONPATH=$(pwd) 
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
```

## Inicializar Airflow

```bash
airflow standalone
```

En el output de `airflow standalone` buscar la contrase√±a para loguearse. Ej:
```
standalone | Airflow is ready
standalone | Login with username: admin  password: pPr9XXxFzgrgGd6U
```


## Ejecutar el DAG

1. Coloca/actualiza el archivo `data/raw/transactions_YYYYMMDD.csv`.


3. Desde la UI o CLI dispara el DAG usando la fecha deseada:

```bash
airflow dags trigger medallion_pipeline --run-id manual_$(date +%s)
```

El DAG ejecutar√°:

- `bronze_clean`: llama a `clean_daily_transactions` para crear `data/clean/transactions_<ds>_clean.parquet`.
- `silver_dbt_run`: ejecuta `dbt run` apuntando al proyecto `dbt/` y carga la data en DuckDB.
- `gold_dbt_tests`: corre `dbt test` y escribe `data/quality/dq_results_<ds>.json` con el status (`passed`/`failed`).

Si un test falla, el archivo igual se genera y el task termina en error para facilitar el monitoreo.

## Ejecutar dbt manualmente

```bash
cd dbt
dbt run
DBT_PROFILES_DIR=../profiles dbt test
```

Aseg√∫rate de exportar `CLEAN_DIR`, `DS_NODASH` y `DUCKDB_PATH` si necesitas sobreescribir valores por defecto:

```bash
export CLEAN_DIR=$(pwd)/../data/clean
export DS_NODASH=20251205
export DUCKDB_PATH=$(pwd)/../warehouse/medallion.duckdb
```

## Observabilidad de Data Quality

Cada corrida crea `data/quality/dq_results_<ds>.json` similar a:

```json
{
  "ds_nodash": "20251205",
  "status": "passed",
  "stdout": "...",
  "stderr": ""
}
```

Ese archivo puede ser ingerido por otras herramientas para auditor√≠a o alertas.


## Verificaci√≥n de resultados por capa

### Bronze
1. Revisa que exista el parquet m√°s reciente:
    ```bash
    $ find data/clean/ | grep transactions_*
    data/clean/transactions_20251201_clean.parquet
    ```
2. Inspecciona las primeras filas para confirmar la limpieza aplicada:
    ```bash
    duckdb -c "
      SELECT *
      FROM read_parquet('data/clean/transactions_20251201_clean.parquet')
      LIMIT 5;
    "
    ```

### Silver
1. Abre el warehouse y lista las tablas creadas por dbt:
    ```bash
    duckdb warehouse/medallion.duckdb -c ".tables"
    ```
2. Ejecuta consultas puntuales para validar c√°lculos intermedios:
    ```bash
    duckdb warehouse/medallion.duckdb -c "
      SELECT *
      FROM fct_customer_transactions
      LIMIT 10;
    "
    ```

### Gold
1. Revisa que exista el parquet m√°s reciente:
    ```bash
    $ find data/quality/*.json
    data/quality/dq_results_20251201.json
    ```

2. Confirma la generaci√≥n del archivo de data quality:
    ```bash
    cat data/quality/dq_results_20251201.json | jq
    ```

3. En caso de fallos, inspecciona `stderr` dentro del mismo JSON o revisa los logs del task en la UI/CLI de Airflow para identificar la prueba que report√≥ error.


## Formato y linting

Usa las herramientas incluidas en `requirements.txt` para mantener un estilo consistente y detectar problemas antes de ejecutar el DAG.

### Black (formateo)

Aplica Black sobre los m√≥dulos de Python del proyecto. A√±ade rutas extra si incorporas nuevos paquetes.

```bash
black dags include
```

### isort (orden de imports)

Ordena autom√°ticamente los imports para evitar diffs innecesarios y mantener un estilo coherente.

```bash
isort dags include
```

### Pylint (est√°tico)

Ejecuta Pylint sobre las mismas carpetas para detectar errores comunes y mejorar la calidad del c√≥digo.

```bash
pylint dags/*.py include/*.py
```

Para ejecutar ambos comandos de una vez puedes usar:

```bash
isort dags include && black dags include && pylint dags/*.py include/*.py
```

## TODOs
Necesarios para completar el workflow:
- [x] Implementar tareas de Airflow.
- [x] Implementar modelos de dbt seg√∫n cada archivo schema.yml.
- [x] Implementar pruebas de dbt para asegurar que las tablas gold est√©n correctas.
- [x] Documentar mejoras posibles para el proceso considerado aspectos de escalabilidad y modelado de datos.
Nice to hace:
- [x] Manejar el caso que no haya archivos para el dia indicado.
- - Se soluciona generando un archivo vacio con las columas indicadas
